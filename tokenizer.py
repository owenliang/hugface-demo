from tokenizers.normalizers import BertNormalizer
from tokenizers.pre_tokenizers import BertPreTokenizer
from tokenizers.models import WordPiece
from tokenizers import Tokenizer
from tokenizers.trainers import WordPieceTrainer
from tokenizers.processors import TemplateProcessing

TRAIN=False 

sentence='å¤§å®¶å¥½,æ¬¢è¿æ¥åˆ°å°é±¼å„¿teacherçš„ç›´æ’­é—´'

if TRAIN:
    # step1 normalizer: ä¸­æ–‡æŒ‰å­—ï¼Œç¬¦å·æŒ‰å­—ï¼Œè‹±æ–‡æŒ‰è¯ï¼Œé¢„å¤„ç†ä¸€ä¸‹å­—ç¬¦ä¸²
    bertNormalizer=BertNormalizer()
    sentence=bertNormalizer.normalize_str(sentence)
    print('BertNormlizer:', sentence)

    # step2 pre_tokenizer: æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†è¯
    bertPreTokenizer=BertPreTokenizer()
    tokens=bertPreTokenizer.pre_tokenize_str(sentence)
    print('bertPreTokenizer:', tokens)

    # step3 modelï¼šè®­ç»ƒåˆ†è¯æ¨¡å‹ï¼ˆå¯¹æ¯ä¸ªè¯æ‹†å­—æ¯ï¼Œç„¶åä¸æ–­åˆå¹¶ï¼‰
    tokenizer=Tokenizer(WordPiece(unk_token='[UNK]'))
    tokenizer.normalizer=bertNormalizer
    tokenizer.pre_tokenizer=bertPreTokenizer

    trainer=WordPieceTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]) #,vocab_size=30000)
    tokenizer.train(['wikitext-103-raw/wiki.train.raw'],trainer)
    tokenizer.save('tokenizer-wiki.json')
else:
    # åŸºäºwikiè‡ªè®­ç»ƒçš„åˆ†è¯å™¨
    tokenizer=Tokenizer.from_file('tokenizer-wiki.json')
    tokenizer.post_processor=TemplateProcessing(
        single='[CLS] $A [SEP]',
        pair="[CLS] $A [SEP] $B:1 [SEP]:1",
        special_tokens=[
            ("[CLS]", tokenizer.token_to_id("[CLS]")),
            ("[SEP]", tokenizer.token_to_id("[SEP]")),
        ]
    )
    output=tokenizer.encode(sentence)
    print('wiki encode result...', 'tokens:', output.tokens, 'ids:', output.ids)

    # æ¼”ç¤ºåè§£
    print('decode:', tokenizer.decode(output.ids))
    print('token2id:', tokenizer.token_to_id('å°'))
    print('id2token:', tokenizer.id_to_token(2018))

    # å¯¹é½å¤šä¸ªsentenceé•¿åº¦,ç”Ÿæˆç›¸åº”çš„æ³¨æ„åŠ›åˆ†æ•°æ©ç 
    tokenizer.enable_padding(pad_token='[PAD]')

    # æ‰¹é‡åˆ†è¯ï¼ˆä¸ºåç»­è¾“å…¥1ä¸ªå¥å­çš„NLPæ¨¡å‹ï¼‰
    output_list=tokenizer.encode_batch(["Hello, y'all!", "How are you ğŸ˜ ?"], )
    for output in output_list:
        print('pretrain batch encode result...', 'tokens:', output.tokens, 'ids:', output.ids, 'mask:',output.attention_mask)

    # æ‰¹é‡åˆ†è¯ï¼ˆä¸ºåç»­è¾“å…¥2ä¸ªå¥å­çš„NLPæ¨¡å‹ï¼‰
    output_list = tokenizer.encode_batch(
        [("Hello, y'all!", "How are you ğŸ˜ ?"), ("Hello to you too!", "I'm fine, thank you!")]
    )
    for output in output_list:
        print('pretrain batch encode pair result...', 'tokens:', output.tokens, 'ids:', output.ids)

    ############################################
    # é¢„è®­ç»ƒçš„åˆ†è¯å™¨(è¯æ›´å¤šæ›´å…¨)
    tokenizer=Tokenizer.from_pretrained('bert-base-chinese')
    output=tokenizer.encode(sentence)
    print('pretrain encode result...', 'tokens:', output.tokens, 'ids:', output.ids)